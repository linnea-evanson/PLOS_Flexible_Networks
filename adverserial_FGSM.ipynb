{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from network_definitions import VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "\n",
    "epsilons = [0, .05, .1, .15, .2, .25, .3]\n",
    "\n",
    "mean_cifar10 = [0.485, 0.456, 0.406]   \n",
    "std_cifar10 = [0.229, 0.224, 0.225]\n",
    "batch_size = 100\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean_cifar10,std_cifar10),\n",
    "])\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='../FlexibleCNNs/data', train=False, download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "\n",
    "print(\"CUDA Available: \",torch.cuda.is_available())\n",
    "use_cuda=True\n",
    "device = torch.device(\"cuda\" if (use_cuda and torch.cuda.is_available()) else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fgsm_attack(image, epsilon, data_grad):\n",
    "\n",
    "    sign_data_grad = data_grad.sign()     # Collect the element-wise sign of the data gradient\n",
    "    perturbed_image = image + epsilon*sign_data_grad # adjusting each pixel of the input image\n",
    "    perturbed_image = torch.clamp(perturbed_image, -2.117, 2.64)     # adding clipping to maintain [0,1] range\n",
    "\n",
    "    return perturbed_image\n",
    "\n",
    "def test( model, device, test_loader, epsilon ):\n",
    "\n",
    "    correct = 0\n",
    "    adv_examples = []\n",
    "\n",
    "    for data, target in test_loader:\n",
    "\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        data.requires_grad = True\n",
    "\n",
    "        output = model(data)\n",
    "        init_pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "\n",
    "        loss = F.nll_loss(output, target)\n",
    "\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        data_grad = data.grad.data\n",
    "\n",
    "        perturbed_data = fgsm_attack(data, epsilon, data_grad)\n",
    "\n",
    "        output = model(perturbed_data)  # Re-classify the perturbed image\n",
    "\n",
    "        final_pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        for i in range(0,len(final_pred)-1):\n",
    "            if final_pred[i].sub(target[i]) == 0:\n",
    "                correct += 1\n",
    "                # Special case for saving 0 epsilon examples\n",
    "                if (epsilon == 0) and (len(adv_examples) < 5):\n",
    "                    adv_ex = perturbed_data.squeeze().detach().cpu().numpy()\n",
    "                    adv_examples.append( (init_pred[i], final_pred[i], adv_ex) )\n",
    "            else:\n",
    "                # Save some adv examples for visualization later\n",
    "                if len(adv_examples) < 5:\n",
    "                    adv_ex = perturbed_data.squeeze().detach().cpu().numpy()\n",
    "                    adv_examples.append( (init_pred[i], final_pred[i], adv_ex) )\n",
    "\n",
    "    # Calculate final accuracy for this epsilon\n",
    "    final_acc = correct/float(len(test_loader))\n",
    "    print(\"Epsilon: {}\\tTest Accuracy = {} / {} = {}\".format(epsilon, correct, len(test_loader), final_acc))\n",
    "\n",
    "    return final_acc, adv_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#---------------------------------MAIN LOOP TO FIND AVERAGE----------------------------------------------\n",
    "\n",
    "# Loop to run for 10 iterations then find the average and standard error of the mean:\n",
    "accuracies = np.zeros((10, len(epsilons))) \n",
    "\n",
    "for i in range(10):\n",
    "    print(\"-------Iteration:\",i,\"--------\")\n",
    "    model = VGG16().to(device)\n",
    "    state = torch.load('./models/model_name/saved_weights.pth')\n",
    "    model.load_state_dict(state['model'])\n",
    "    model.eval()\n",
    "\n",
    "    # Evaluate model\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            outputs = model(images.cuda())\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels.cuda()).sum().item()\n",
    "\n",
    "    print('Accuracy on the test dataset is: %f %%' % (100 * correct / total))\n",
    "\n",
    "    # Run test for each epsilon\n",
    "    for j, eps in enumerate(epsilons):\n",
    "        print(\"j:\", j,\"   eps:\", eps)\n",
    "        acc, ex = test(model, device, testloader, eps)\n",
    "        print(\"acc:\", acc)\n",
    "        accuracies[i,j] = acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save\n",
    "np.save(f\"FGSM_acc_10iterations_model_{model.name}\", accuracies)\n",
    "\n",
    "#Calculate averages:\n",
    "avg_acc = np.mean(accuracies, axis = 0)\n",
    "\n",
    "#Calculate error:\n",
    "sem = np.std(accuracies, axis = 0) / np.sqrt(10)   #standard error over 10 samples\n",
    "\n",
    "#Save average values:\n",
    "np.save(f\"FGSM_acc_average_{model.name}\", avg_acc)\n",
    "np.save(f\"FGSM_acc_sem_{model.name}\", sem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot average:\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.plot(epsilons, avg_acc, \"*-\", label = \"Flexible VGG16\")\n",
    "plt.yticks(np.arange(0, 1.1, step=0.1))\n",
    "plt.xticks(np.arange(0, .35, step=0.05))\n",
    "plt.title(\"FGSM Testing Accuracy vs Epsilon\")\n",
    "plt.xlabel(\"Epsilon\")\n",
    "plt.ylabel(\"Testing Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot average with error bars:\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.errorbar(epsilons, avg_acc, yerr= sem)\n",
    "plt.yticks(np.arange(0, 1.1, step=0.1))\n",
    "plt.xticks(np.arange(0, .35, step=0.05))\n",
    "plt.title(\"FGSM Testing Accuracy vs Epsilon\")\n",
    "plt.xlabel(\"Epsilon\")\n",
    "plt.ylabel(\"Testing Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
